Prepare data
    Read in
    Split into training and test

Read topology of network

Initialize the parameters
    for each layer

        create matrix of weights with given layer size
        create matrix of bias    with given layer size

        initialize matrix of weights with given layer size
        initialize matrix of bias    with given layer size

        update paramteter dict , key = weight + layer , value = weight matrix for layer
        update paramteter dict , key = bias   + layer , value = bias matrix for layer

Repeat for given num of iterations

    Forward propagation

        for each layers (L-1)
            get inputs matrix
            get weight matrix
            get bias   matrix
            set activation function

            Propogate forward :
                Z[l]    = get  pre-activation values : Z = (W @ A) + b
                l_cache = save pre-activation cache  : linear_cache = tuple(A,W,b)

                according to activation function : Compute g(Z[l])
                    A       = get  post-activation values : activate (Z)
                    a_cache = save post-activation values : activation_cache = (A,W,b)

                save cache tuple = ( pre-activation cache , post-activation cache )
                append cache tuple to list to caches

        for last layer (L)
        repeat above steps , propogate forward , with activation sigmoid
        append cache tuple to caches list


    Calculate Loss

    Back propogation


    # caches = list  of tuples of tuples : cached values for each layer
    # cache  = tuple of tuples           : ( pre-activation cache , post-activation cache )
    # cache  = tuple of tuples           : ( linear cache         , activation cache      )
    # cache  = tuple of tuples           : ( (Z      , W , b)     , (A,W,b)               )
    # cache  = tuple of tuples           : ( (A_prev , W , b)     , (A,W,b)               )

    # If g(.) is the activation function , sigmoid_backward , relu_backward compute dZ[l] = dA[l-1] * g'[l]

	dZ      = activation_backward   ( dA , activation_cache )
    dW      = (1/m) * dZ @ A_prev.T
    db      = (1/m) * sum(dZ)
    dA_prev = W.T @ dZ







        Propogate backward :
        Compute gradients  : dW[l] , db[l]

    Update parameters

Predict labels using the trained parameters

